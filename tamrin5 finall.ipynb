{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#####متن اول(Aggregation.txt)"
      ],
      "metadata": {
        "id": "wiQWU8spYEHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####خوشه بندی agglomorative"
      ],
      "metadata": {
        "id": "s-0CPExZYUq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import pdist\n",
        "link = [\"http://cs.joensuu.fi/sipu/datasets/Aggregation.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "num_points = len(values)\n",
        "weight_matrix = np.zeros((num_points, num_points))\n",
        "for i in range(num_points):\n",
        "    for j in range(i + 1, num_points):\n",
        "        weight_matrix[i, j] = gaussian_kernel(values[i], values[j], sigma)\n",
        "        weight_matrix[j, i] = weight_matrix[i, j]\n",
        "agglomerative_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='average')\n",
        "agglomerative_clustering.fit(values)\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "linkage_matrix = agglomerative_clustering.children_\n",
        "weight_matrix = weight_matrix\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"cost_dasgupta\", cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnKow5hRv3ld",
        "outputId": "d97565a9-47f0-411b-ea3d-281b80f5e08d"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost_dasgupta 186366.61059553578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=3"
      ],
      "metadata": {
        "id": "220iHeqcYdKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"http://cs.joensuu.fi/sipu/datasets/Aggregation.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k = 3\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=3\",cost)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbH_EnnEUSAK",
        "outputId": "f67b0331-a9fa-454c-897a-881f2b91520d"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=3 994.0855011281172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=5"
      ],
      "metadata": {
        "id": "EQwtNMU2YtG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"http://cs.joensuu.fi/sipu/datasets/Aggregation.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =5\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=5\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nLvjldmUaXp",
        "outputId": "555f9d76-1f03-442e-af29-e94086cf2d5c"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=5 1834.5087769683992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=10"
      ],
      "metadata": {
        "id": "OW5R6WNYY5oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"http://cs.joensuu.fi/sipu/datasets/Aggregation.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =10\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=10\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxQf_u1uYyZY",
        "outputId": "527ca5cc-b4b9-4692-c014-ee3cf7ae6f58"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=10 4015.878531092625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####### جدول مقایسه"
      ],
      "metadata": {
        "id": "g46FTJRQZEl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "link = [\"http://cs.joensuu.fi/sipu/datasets/Aggregation.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_spectral_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def spectral_laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def spectral_second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "def create_agglomerative_graph(values, sigma):\n",
        "    num_points = len(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j in range(i + 1, num_points):\n",
        "            weight_matrix[i, j] = gaussian_kernel(values[i], values[j], sigma)\n",
        "            weight_matrix[j, i] = weight_matrix[i, j]\n",
        "    return weight_matrix\n",
        "agglomerative_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='average')\n",
        "agglomerative_clustering.fit(values)\n",
        "dataset[\"new_labels\"] = agglomerative_clustering.labels_\n",
        "linkage_matrix_agglomerative = agglomerative_clustering.children_\n",
        "weight_matrix_agglomerative = create_agglomerative_graph(values, sigma)\n",
        "cost_agglomerative = spectral_dasgupta_cost(linkage_matrix_agglomerative, weight_matrix_agglomerative)\n",
        "k_values = [3, 5, 10]\n",
        "cost_spectral_values = []\n",
        "for k in k_values:\n",
        "    weight_matrix_spectral = create_spectral_graph(values, k, sigma)\n",
        "    laplacian_spectral = spectral_laplacian_matrix(weight_matrix_spectral)\n",
        "    second_eig_spectral = spectral_second_eigenvector(laplacian_spectral)\n",
        "    linkage_matrix_spectral = linkage(values, method='ward')\n",
        "    cost_spectral = spectral_dasgupta_cost(linkage_matrix_spectral, weight_matrix_spectral)\n",
        "    cost_spectral_values.append(cost_spectral)\n",
        "data = {\"Method\": [\"Spectral Clustering (k=3)\", \"Spectral Clustering (k=5)\", \"Spectral Clustering (k=10)\", \"Agglomerative Clustering\"],\n",
        "    \"Dasgupta Cost\": cost_spectral_values + [cost_agglomerative]}\n",
        "df_result = pd.DataFrame(data)\n",
        "print(df_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezesr4DOaigB",
        "outputId": "efbec3da-4303-4dd7-cd76-e2c34d858e9d"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       Method  Dasgupta Cost\n",
            "0   Spectral Clustering (k=3)     994.085501\n",
            "1   Spectral Clustering (k=5)    1834.508777\n",
            "2  Spectral Clustering (k=10)    4015.878531\n",
            "3    Agglomerative Clustering  186366.610596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############ متن دوم(Compound.txt)"
      ],
      "metadata": {
        "id": "Ku7OVHkZdkQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import pdist\n",
        "link = [\"https://cs.joensuu.fi/sipu/datasets/Compound.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "num_points = len(values)\n",
        "weight_matrix = np.zeros((num_points, num_points))\n",
        "for i in range(num_points):\n",
        "    for j in range(i + 1, num_points):\n",
        "        weight_matrix[i, j] = gaussian_kernel(values[i], values[j], sigma)\n",
        "        weight_matrix[j, i] = weight_matrix[i, j]\n",
        "agglomerative_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='average')\n",
        "agglomerative_clustering.fit(values)\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "linkage_matrix = agglomerative_clustering.children_\n",
        "weight_matrix = weight_matrix\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"cost_dasgupta\", cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24Jlt0_DdP7a",
        "outputId": "f6dc5a4c-e496-42e4-f70a-09d0a49afbf8"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost_dasgupta 48099.511786097166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=3"
      ],
      "metadata": {
        "id": "MLTGKDt0exVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/Compound.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k = 3\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=3\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TopOegmMekab",
        "outputId": "bc463c00-0cc6-41e1-93c2-081742fcb0ae"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=3 531.3772012032567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=5"
      ],
      "metadata": {
        "id": "qOjbTVxfe-IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/Compound.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =5\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=5\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMJ2Yz3DfI6J",
        "outputId": "ed05a33b-331a-4202-dfd0-c3b7c2b19d18"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=5 954.3126286591983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=10"
      ],
      "metadata": {
        "id": "ADjYZGCOfj4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/Compound.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =10\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=10\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3SkSedkfgtZ",
        "outputId": "d504cf3a-c001-4a69-c84d-aea04a1c7ac3"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=10 2164.780801966074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####### جدول مقایسه"
      ],
      "metadata": {
        "id": "BX5ewV8XgZ6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "link = [\"https://cs.joensuu.fi/sipu/datasets/Compound.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_spectral_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def spectral_laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def spectral_second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "def create_agglomerative_graph(values, sigma):\n",
        "    num_points = len(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j in range(i + 1, num_points):\n",
        "            weight_matrix[i, j] = gaussian_kernel(values[i], values[j], sigma)\n",
        "            weight_matrix[j, i] = weight_matrix[i, j]\n",
        "    return weight_matrix\n",
        "agglomerative_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='average')\n",
        "agglomerative_clustering.fit(values)\n",
        "dataset[\"new_labels\"] = agglomerative_clustering.labels_\n",
        "linkage_matrix_agglomerative = agglomerative_clustering.children_\n",
        "weight_matrix_agglomerative = create_agglomerative_graph(values, sigma)\n",
        "cost_agglomerative = spectral_dasgupta_cost(linkage_matrix_agglomerative, weight_matrix_agglomerative)\n",
        "k_values = [3, 5, 10]\n",
        "cost_spectral_values = []\n",
        "for k in k_values:\n",
        "    weight_matrix_spectral = create_spectral_graph(values, k, sigma)\n",
        "    laplacian_spectral = spectral_laplacian_matrix(weight_matrix_spectral)\n",
        "    second_eig_spectral = spectral_second_eigenvector(laplacian_spectral)\n",
        "    linkage_matrix_spectral = linkage(values, method='ward')\n",
        "    cost_spectral = spectral_dasgupta_cost(linkage_matrix_spectral, weight_matrix_spectral)\n",
        "    cost_spectral_values.append(cost_spectral)\n",
        "data = {\"Method\": [\"Spectral Clustering (k=3)\", \"Spectral Clustering (k=5)\", \"Spectral Clustering (k=10)\", \"Agglomerative Clustering\"],\n",
        "    \"Dasgupta Cost\": cost_spectral_values + [cost_agglomerative]}\n",
        "df_result = pd.DataFrame(data)\n",
        "print(df_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh3KJPMBfvvA",
        "outputId": "352c9170-536b-4775-ecf3-d37384a6d643"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       Method  Dasgupta Cost\n",
            "0   Spectral Clustering (k=3)     531.377201\n",
            "1   Spectral Clustering (k=5)     954.312629\n",
            "2  Spectral Clustering (k=10)    2164.780802\n",
            "3    Agglomerative Clustering   48099.511786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### متن سوم(pathbased.txt)"
      ],
      "metadata": {
        "id": "4HEOjdVWgevU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####خوشه بندی agglomorative"
      ],
      "metadata": {
        "id": "xHg1e7avmtJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import pdist\n",
        "link = [\"https://cs.joensuu.fi/sipu/datasets/pathbased.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "num_points = len(values)\n",
        "weight_matrix = np.zeros((num_points, num_points))\n",
        "for i in range(num_points):\n",
        "    for j in range(i + 1, num_points):\n",
        "        weight_matrix[i, j] = gaussian_kernel(values[i], values[j], sigma)\n",
        "        weight_matrix[j, i] = weight_matrix[i, j]\n",
        "agglomerative_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='average')\n",
        "agglomerative_clustering.fit(values)\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "linkage_matrix = agglomerative_clustering.children_\n",
        "weight_matrix = weight_matrix\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"cost_dasgupta\", cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc8kuA_HiAY8",
        "outputId": "006069e5-1b2c-44ca-87db-7565c2031c33"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost_dasgupta 27067.34238239806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=3"
      ],
      "metadata": {
        "id": "Tad4MBFnlAEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/pathbased.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =3\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=3\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X49tDP-nlO-B",
        "outputId": "96334dcb-ca28-4957-bc83-50c8a767f768"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=3 383.8850705514417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=5"
      ],
      "metadata": {
        "id": "LK-gBXmtmBDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/pathbased.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =5\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=5\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDbnAt7Xl_C0",
        "outputId": "5dd7de3d-9a55-499e-caba-28be6785dd91"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=5 727.4555049320236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=10"
      ],
      "metadata": {
        "id": "0L0j7UrzmFuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/pathbased.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =10\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=10\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cusqGZZHmI-B",
        "outputId": "f0d551ec-57b3-49be-ce7b-5511e306cd47"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=10 1562.0117040759308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### جدول مقایسه"
      ],
      "metadata": {
        "id": "xKyTBd8hmMeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "link = [\"https://cs.joensuu.fi/sipu/datasets/pathbased.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_spectral_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def spectral_laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def spectral_second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "def create_agglomerative_graph(values, sigma):\n",
        "    num_points = len(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j in range(i + 1, num_points):\n",
        "            weight_matrix[i, j] = gaussian_kernel(values[i], values[j], sigma)\n",
        "            weight_matrix[j, i] = weight_matrix[i, j]\n",
        "    return weight_matrix\n",
        "agglomerative_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='average')\n",
        "agglomerative_clustering.fit(values)\n",
        "dataset[\"new_labels\"] = agglomerative_clustering.labels_\n",
        "linkage_matrix_agglomerative = agglomerative_clustering.children_\n",
        "weight_matrix_agglomerative = create_agglomerative_graph(values, sigma)\n",
        "cost_agglomerative = spectral_dasgupta_cost(linkage_matrix_agglomerative, weight_matrix_agglomerative)\n",
        "k_values = [3, 5, 10]\n",
        "cost_spectral_values = []\n",
        "for k in k_values:\n",
        "    weight_matrix_spectral = create_spectral_graph(values, k, sigma)\n",
        "    laplacian_spectral = spectral_laplacian_matrix(weight_matrix_spectral)\n",
        "    second_eig_spectral = spectral_second_eigenvector(laplacian_spectral)\n",
        "    linkage_matrix_spectral = linkage(values, method='ward')\n",
        "    cost_spectral = spectral_dasgupta_cost(linkage_matrix_spectral, weight_matrix_spectral)\n",
        "    cost_spectral_values.append(cost_spectral)\n",
        "data = {\"Method\": [\"Spectral Clustering (k=3)\", \"Spectral Clustering (k=5)\", \"Spectral Clustering (k=10)\", \"Agglomerative Clustering\"],\n",
        "    \"Dasgupta Cost\": cost_spectral_values + [cost_agglomerative]}\n",
        "df_result = pd.DataFrame(data)\n",
        "print(df_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0tFbOhtmOK7",
        "outputId": "525603fc-24d4-4125-a5ed-0348125ddd62"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       Method  Dasgupta Cost\n",
            "0   Spectral Clustering (k=3)     383.885071\n",
            "1   Spectral Clustering (k=5)     727.455505\n",
            "2  Spectral Clustering (k=10)    1562.011704\n",
            "3    Agglomerative Clustering   27067.342382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############# متن چهارم(spiral.txt)"
      ],
      "metadata": {
        "id": "RaIkWnn-q5mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####خوشه بندی agglomorative"
      ],
      "metadata": {
        "id": "LfExNUUlrIC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import pdist\n",
        "link = [\"https://cs.joensuu.fi/sipu/datasets/spiral.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "num_points = len(values)\n",
        "weight_matrix = np.zeros((num_points, num_points))\n",
        "for i in range(num_points):\n",
        "    for j in range(i + 1, num_points):\n",
        "        weight_matrix[i, j] = gaussian_kernel(values[i], values[j], sigma)\n",
        "        weight_matrix[j, i] = weight_matrix[i, j]\n",
        "agglomerative_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='average')\n",
        "agglomerative_clustering.fit(values)\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "linkage_matrix = agglomerative_clustering.children_\n",
        "weight_matrix = weight_matrix\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"cost_dasgupta\", cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfzizkZNrLJq",
        "outputId": "cc4ef6b0-21d4-45ab-cd52-e3f2f02b2793"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost_dasgupta 29415.44910564478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=3"
      ],
      "metadata": {
        "id": "d-2QTnCvrg8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/spiral.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =3\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=3\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0V1aq1qrbqK",
        "outputId": "f92a85a9-cc2b-4cc5-b467-5c48f915c063"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=3 315.5805104355261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=5"
      ],
      "metadata": {
        "id": "ocG0VlhhriAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/spiral.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =5\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=5\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWdBxGc6rmLj",
        "outputId": "ee39a6e8-6b29-4cd7-dec1-60c1e9e2cdb5"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=5 634.8311391937355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=10"
      ],
      "metadata": {
        "id": "LCm0RMuPrzFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/spiral.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =10\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=10\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB4HbF9Jr1pK",
        "outputId": "0ad21d61-b968-46e8-9ca5-81857f907a60"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=10 1544.4077652542774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### جدول مقایسه"
      ],
      "metadata": {
        "id": "1AXiu-pJr8TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "link = [\"https://cs.joensuu.fi/sipu/datasets/spiral.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_spectral_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def spectral_laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def spectral_second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "def create_agglomerative_graph(values, sigma):\n",
        "    num_points = len(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j in range(i + 1, num_points):\n",
        "            weight_matrix[i, j] = gaussian_kernel(values[i], values[j], sigma)\n",
        "            weight_matrix[j, i] = weight_matrix[i, j]\n",
        "    return weight_matrix\n",
        "agglomerative_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='average')\n",
        "agglomerative_clustering.fit(values)\n",
        "dataset[\"new_labels\"] = agglomerative_clustering.labels_\n",
        "linkage_matrix_agglomerative = agglomerative_clustering.children_\n",
        "weight_matrix_agglomerative = create_agglomerative_graph(values, sigma)\n",
        "cost_agglomerative = spectral_dasgupta_cost(linkage_matrix_agglomerative, weight_matrix_agglomerative)\n",
        "k_values = [3, 5, 10]\n",
        "cost_spectral_values = []\n",
        "for k in k_values:\n",
        "    weight_matrix_spectral = create_spectral_graph(values, k, sigma)\n",
        "    laplacian_spectral = spectral_laplacian_matrix(weight_matrix_spectral)\n",
        "    second_eig_spectral = spectral_second_eigenvector(laplacian_spectral)\n",
        "    linkage_matrix_spectral = linkage(values, method='ward')\n",
        "    cost_spectral = spectral_dasgupta_cost(linkage_matrix_spectral, weight_matrix_spectral)\n",
        "    cost_spectral_values.append(cost_spectral)\n",
        "data = {\"Method\": [\"Spectral Clustering (k=3)\", \"Spectral Clustering (k=5)\", \"Spectral Clustering (k=10)\", \"Agglomerative Clustering\"],\n",
        "    \"Dasgupta Cost\": cost_spectral_values + [cost_agglomerative]}\n",
        "df_result = pd.DataFrame(data)\n",
        "print(df_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duo6WpQQr97L",
        "outputId": "cfe9723f-9d5f-4d86-d594-2fd1b59c9e2a"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       Method  Dasgupta Cost\n",
            "0   Spectral Clustering (k=3)     315.580510\n",
            "1   Spectral Clustering (k=5)     634.831139\n",
            "2  Spectral Clustering (k=10)    1544.407765\n",
            "3    Agglomerative Clustering   29415.449106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## متن پنجم(D31.txt)"
      ],
      "metadata": {
        "id": "JrjiZwe2mX15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import pdist\n",
        "link = [\"https://cs.joensuu.fi/sipu/datasets/D31.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "num_points = len(values)\n",
        "weight_matrix = np.zeros((num_points, num_points))\n",
        "for i in range(num_points):\n",
        "    for j in range(i + 1, num_points):\n",
        "        weight_matrix[i, j] = gaussian_kernel(values[i], values[j], sigma)\n",
        "        weight_matrix[j, i] = weight_matrix[i, j]\n",
        "agglomerative_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='average')\n",
        "agglomerative_clustering.fit(values)\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "linkage_matrix = agglomerative_clustering.children_\n",
        "weight_matrix = weight_matrix\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"cost_dasgupta\", cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itqo0HvAm1ay",
        "outputId": "300e7bbd-dc58-4990-8681-853987d655e5"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost_dasgupta 2898836.994617578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=3"
      ],
      "metadata": {
        "id": "jSEmSS9Mm6YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/D31.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =3\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=3\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMF8hQvKm8bC",
        "outputId": "c1961eed-7917-4b04-80cd-e3923dec31b9"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=3 4109.962342313901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=5"
      ],
      "metadata": {
        "id": "-9LVsM91wtSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/D31.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =5\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=5\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttohzpchnRpH",
        "outputId": "7101a093-7b4a-4042-ecd2-d1a7a2074b13"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=5 7855.864800217752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=10"
      ],
      "metadata": {
        "id": "h3J9pO4jsSqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/D31.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =10\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=10\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiHqCQSLneDz",
        "outputId": "7add8063-eb96-429c-e182-39677b619992"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=10 17408.30316642998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### جدول مقایسه"
      ],
      "metadata": {
        "id": "QVUQLbYLqmhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "link = [\"https://cs.joensuu.fi/sipu/datasets/D31.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_spectral_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def spectral_laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def spectral_second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "def create_agglomerative_graph(values, sigma):\n",
        "    num_points = len(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j in range(i + 1, num_points):\n",
        "            weight_matrix[i, j] = gaussian_kernel(values[i], values[j], sigma)\n",
        "            weight_matrix[j, i] = weight_matrix[i, j]\n",
        "    return weight_matrix\n",
        "agglomerative_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='average')\n",
        "agglomerative_clustering.fit(values)\n",
        "dataset[\"new_labels\"] = agglomerative_clustering.labels_\n",
        "linkage_matrix_agglomerative = agglomerative_clustering.children_\n",
        "weight_matrix_agglomerative = create_agglomerative_graph(values, sigma)\n",
        "cost_agglomerative = spectral_dasgupta_cost(linkage_matrix_agglomerative, weight_matrix_agglomerative)\n",
        "k_values = [3, 5, 10]\n",
        "cost_spectral_values = []\n",
        "for k in k_values:\n",
        "    weight_matrix_spectral = create_spectral_graph(values, k, sigma)\n",
        "    laplacian_spectral = spectral_laplacian_matrix(weight_matrix_spectral)\n",
        "    second_eig_spectral = spectral_second_eigenvector(laplacian_spectral)\n",
        "    linkage_matrix_spectral = linkage(values, method='ward')\n",
        "    cost_spectral = spectral_dasgupta_cost(linkage_matrix_spectral, weight_matrix_spectral)\n",
        "    cost_spectral_values.append(cost_spectral)\n",
        "data = {\"Method\": [\"Spectral Clustering (k=3)\", \"Spectral Clustering (k=5)\", \"Spectral Clustering (k=10)\", \"Agglomerative Clustering\"],\n",
        "    \"Dasgupta Cost\": cost_spectral_values + [cost_agglomerative]}\n",
        "df_result = pd.DataFrame(data)\n",
        "print(df_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu2XhJE4pXyT",
        "outputId": "f955fe8c-192c-4a24-abde-88a9738058ee"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       Method  Dasgupta Cost\n",
            "0   Spectral Clustering (k=3)   4.109962e+03\n",
            "1   Spectral Clustering (k=5)   7.855865e+03\n",
            "2  Spectral Clustering (k=10)   1.740830e+04\n",
            "3    Agglomerative Clustering   2.898837e+06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## متن ششم(R15.txt)"
      ],
      "metadata": {
        "id": "MEQ0fz25qp4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####خوشه بندی agglomorative"
      ],
      "metadata": {
        "id": "wRvG2xDZs0eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import pdist\n",
        "link = [\"https://cs.joensuu.fi/sipu/datasets/R15.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "num_points = len(values)\n",
        "weight_matrix = np.zeros((num_points, num_points))\n",
        "for i in range(num_points):\n",
        "    for j in range(i + 1, num_points):\n",
        "        weight_matrix[i, j] = gaussian_kernel(values[i], values[j], sigma)\n",
        "        weight_matrix[j, i] = weight_matrix[i, j]\n",
        "agglomerative_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='average')\n",
        "agglomerative_clustering.fit(values)\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "linkage_matrix = agglomerative_clustering.children_\n",
        "weight_matrix = weight_matrix\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"cost_dasgupta\", cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHNXZF2UslRr",
        "outputId": "156a49b3-81e4-4da0-cdb4-18a0be3ea725"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost_dasgupta 108681.25618708035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=3"
      ],
      "metadata": {
        "id": "zI3xrhz0swRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/R15.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =3\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=3\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvfnr0Ygs3Wi",
        "outputId": "ad08b88f-8970-4f9f-9ecc-f88edfa34c15"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=3 807.6099809488633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=5"
      ],
      "metadata": {
        "id": "_66T4bwHtI47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/R15.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =5\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=5\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAquNwEytEIc",
        "outputId": "e3d2e7ba-b41a-4564-b45b-ab7b6dbe6633"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=5 1565.7693710057847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=10"
      ],
      "metadata": {
        "id": "EtpDtae-tLqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/R15.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =10\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=10\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0v1dIB5tOAc",
        "outputId": "c7b7d8d2-364c-4a33-cbb7-740823795421"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=10 3493.96803105565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### جدول مقایسه"
      ],
      "metadata": {
        "id": "_8BXHCrxtScO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "link = [\"https://cs.joensuu.fi/sipu/datasets/R15.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_spectral_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def spectral_laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def spectral_second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "def create_agglomerative_graph(values, sigma):\n",
        "    num_points = len(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j in range(i + 1, num_points):\n",
        "            weight_matrix[i, j] = gaussian_kernel(values[i], values[j], sigma)\n",
        "            weight_matrix[j, i] = weight_matrix[i, j]\n",
        "    return weight_matrix\n",
        "agglomerative_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='average')\n",
        "agglomerative_clustering.fit(values)\n",
        "dataset[\"new_labels\"] = agglomerative_clustering.labels_\n",
        "linkage_matrix_agglomerative = agglomerative_clustering.children_\n",
        "weight_matrix_agglomerative = create_agglomerative_graph(values, sigma)\n",
        "cost_agglomerative = spectral_dasgupta_cost(linkage_matrix_agglomerative, weight_matrix_agglomerative)\n",
        "k_values = [3, 5, 10]\n",
        "cost_spectral_values = []\n",
        "for k in k_values:\n",
        "    weight_matrix_spectral = create_spectral_graph(values, k, sigma)\n",
        "    laplacian_spectral = spectral_laplacian_matrix(weight_matrix_spectral)\n",
        "    second_eig_spectral = spectral_second_eigenvector(laplacian_spectral)\n",
        "    linkage_matrix_spectral = linkage(values, method='ward')\n",
        "    cost_spectral = spectral_dasgupta_cost(linkage_matrix_spectral, weight_matrix_spectral)\n",
        "    cost_spectral_values.append(cost_spectral)\n",
        "data = {\"Method\": [\"Spectral Clustering (k=3)\", \"Spectral Clustering (k=5)\", \"Spectral Clustering (k=10)\", \"Agglomerative Clustering\"],\n",
        "    \"Dasgupta Cost\": cost_spectral_values + [cost_agglomerative]}\n",
        "df_result = pd.DataFrame(data)\n",
        "print(df_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0bQtyImtUUe",
        "outputId": "447307e0-f687-453b-d26b-83562ee04201"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       Method  Dasgupta Cost\n",
            "0   Spectral Clustering (k=3)     807.609981\n",
            "1   Spectral Clustering (k=5)    1565.769371\n",
            "2  Spectral Clustering (k=10)    3493.968031\n",
            "3    Agglomerative Clustering  108681.256187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######## متن هفتم(jain.txt)"
      ],
      "metadata": {
        "id": "_RnqySN9te9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####خوشه بندی agglomorative"
      ],
      "metadata": {
        "id": "OdTb9GTlt02r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import pdist\n",
        "link = [\"https://cs.joensuu.fi/sipu/datasets/jain.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "num_points = len(values)\n",
        "weight_matrix = np.zeros((num_points, num_points))\n",
        "for i in range(num_points):\n",
        "    for j in range(i + 1, num_points):\n",
        "        weight_matrix[i, j] = gaussian_kernel(values[i], values[j], sigma)\n",
        "        weight_matrix[j, i] = weight_matrix[i, j]\n",
        "agglomerative_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='average')\n",
        "agglomerative_clustering.fit(values)\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "linkage_matrix = agglomerative_clustering.children_\n",
        "weight_matrix = weight_matrix\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"cost_dasgupta\", cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mRrR8-0toYI",
        "outputId": "01f662f3-fb1d-4f9c-91ce-7ba0ef1dac4f"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost_dasgupta 42093.84241554653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=3"
      ],
      "metadata": {
        "id": "9aLuzIldt4XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/jain.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =3\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=3\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF7B5nZ1t77-",
        "outputId": "d205e56a-e2a4-4985-ef94-aea947f0a34e"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=3 478.1763672838005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=5"
      ],
      "metadata": {
        "id": "FwoAyBzZuNCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/jain.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =5\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=5\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soV38EPKuG7K",
        "outputId": "6e12351b-e518-46b6-aaa9-1c4f1418b0af"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=5 917.3398473948137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=10"
      ],
      "metadata": {
        "id": "J1xgIu-suP_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/jain.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =10\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=10\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfH_iih_uSP3",
        "outputId": "322410a9-99a1-4b00-a932-cec316b8efdc"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=10 1999.1001318004778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### جدول مقایسه"
      ],
      "metadata": {
        "id": "U0Mm7hoUuXe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "link = [\"https://cs.joensuu.fi/sipu/datasets/jain.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_spectral_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def spectral_laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def spectral_second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "def create_agglomerative_graph(values, sigma):\n",
        "    num_points = len(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j in range(i + 1, num_points):\n",
        "            weight_matrix[i, j] = gaussian_kernel(values[i], values[j], sigma)\n",
        "            weight_matrix[j, i] = weight_matrix[i, j]\n",
        "    return weight_matrix\n",
        "agglomerative_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='average')\n",
        "agglomerative_clustering.fit(values)\n",
        "dataset[\"new_labels\"] = agglomerative_clustering.labels_\n",
        "linkage_matrix_agglomerative = agglomerative_clustering.children_\n",
        "weight_matrix_agglomerative = create_agglomerative_graph(values, sigma)\n",
        "cost_agglomerative = spectral_dasgupta_cost(linkage_matrix_agglomerative, weight_matrix_agglomerative)\n",
        "k_values = [3, 5, 10]\n",
        "cost_spectral_values = []\n",
        "for k in k_values:\n",
        "    weight_matrix_spectral = create_spectral_graph(values, k, sigma)\n",
        "    laplacian_spectral = spectral_laplacian_matrix(weight_matrix_spectral)\n",
        "    second_eig_spectral = spectral_second_eigenvector(laplacian_spectral)\n",
        "    linkage_matrix_spectral = linkage(values, method='ward')\n",
        "    cost_spectral = spectral_dasgupta_cost(linkage_matrix_spectral, weight_matrix_spectral)\n",
        "    cost_spectral_values.append(cost_spectral)\n",
        "data = {\"Method\": [\"Spectral Clustering (k=3)\", \"Spectral Clustering (k=5)\", \"Spectral Clustering (k=10)\", \"Agglomerative Clustering\"],\n",
        "    \"Dasgupta Cost\": cost_spectral_values + [cost_agglomerative]}\n",
        "df_result = pd.DataFrame(data)\n",
        "print(df_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_lN-dz3uavt",
        "outputId": "9f145f49-0f72-491d-812b-06d34c7876b9"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       Method  Dasgupta Cost\n",
            "0   Spectral Clustering (k=3)     478.176367\n",
            "1   Spectral Clustering (k=5)     917.339847\n",
            "2  Spectral Clustering (k=10)    1999.100132\n",
            "3    Agglomerative Clustering   42093.842416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## متن هشتم(flame.txt)"
      ],
      "metadata": {
        "id": "lXXwOBveumYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####خوشه بندی agglomorative"
      ],
      "metadata": {
        "id": "I_r_xDTZuvtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import pdist\n",
        "link = [\"https://cs.joensuu.fi/sipu/datasets/flame.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "num_points = len(values)\n",
        "weight_matrix = np.zeros((num_points, num_points))\n",
        "for i in range(num_points):\n",
        "    for j in range(i + 1, num_points):\n",
        "        weight_matrix[i, j] = gaussian_kernel(values[i], values[j], sigma)\n",
        "        weight_matrix[j, i] = weight_matrix[i, j]\n",
        "agglomerative_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='average')\n",
        "agglomerative_clustering.fit(values)\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "linkage_matrix = agglomerative_clustering.children_\n",
        "weight_matrix = weight_matrix\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"cost_dasgupta\", cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHxSVqhluyLP",
        "outputId": "c838c925-a622-42cc-e69f-2e6eccdf95db"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost_dasgupta 17316.747367472835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=3"
      ],
      "metadata": {
        "id": "nikHxK_BvFYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/flame.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =3\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=3\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPLfq4eYvB9V",
        "outputId": "7d8e7ba8-90c7-4a18-8de8-b80953cb1142"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=3 309.87232631889486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=5"
      ],
      "metadata": {
        "id": "GfeJwzH-vNKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/flame.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =5\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=5\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLijOaqmvPIF",
        "outputId": "f13ebe1c-d95c-44fc-ca75-a9e1c00b8bce"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=5 561.719775069958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### خوشه بندی سلسله مراتبی طیفی برای k=10"
      ],
      "metadata": {
        "id": "uY77oS3dvWMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "link= [\"https://cs.joensuu.fi/sipu/datasets/flame.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_cut(values, weight_matrix, k, sigma):\n",
        "    W = create_graph(values, k, sigma)\n",
        "    L = laplacian_matrix(W)\n",
        "    second_eig = second_eigenvector(L)\n",
        "    partition = np.sign(second_eig)\n",
        "    return partition, second_eig\n",
        "def hierarchical_spectral_clustering(values, k, sigma):\n",
        "    weight_matrix = create_graph(values, k, sigma)\n",
        "    linkage_matrix = linkage(values, method='ward')\n",
        "    return linkage_matrix, weight_matrix\n",
        "def node_distance(linkage_matrix, point1, point2, num_points):\n",
        "    node1, node2 = point1, point2\n",
        "    if node1 < num_points and node2 < num_points:\n",
        "        return 1\n",
        "    while node1 >= num_points or node2 >= num_points:\n",
        "        if node1 >= num_points:\n",
        "            node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "        elif node2 >= num_points:\n",
        "            node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "        else:\n",
        "            if linkage_matrix[node1 - num_points, 0] == linkage_matrix[node2 - num_points, 0]:\n",
        "                break\n",
        "            if linkage_matrix[node1 - num_points, 2] < linkage_matrix[node2 - num_points, 2]:\n",
        "                node1 = int(linkage_matrix[node1 - num_points, 0])\n",
        "            else:\n",
        "                node2 = int(linkage_matrix[node2 - num_points, 0])\n",
        "    return linkage_matrix[node1 - num_points, 2]\n",
        "def dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "k =10\n",
        "linkage_matrix, weight_matrix = hierarchical_spectral_clustering(values, k, sigma)\n",
        "cost = dasgupta_cost(linkage_matrix, weight_matrix)\n",
        "print(\"dasgupta_cost for k=10\",cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XrYowRKvS_H",
        "outputId": "c0032af8-b747-424a-8d82-ea30502760eb"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dasgupta_cost for k=10 1250.8872677337697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### جدول مقایسه"
      ],
      "metadata": {
        "id": "X_7kKxWCvavO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "link = [\"https://cs.joensuu.fi/sipu/datasets/flame.txt\"]\n",
        "numbers = []\n",
        "read = requests.get(link[0])\n",
        "content = read.text\n",
        "data = StringIO(content)\n",
        "df = pd.read_csv(data, sep=\"\\s+\", header=None, names=[\"x1\", \"x2\", \"labels\"])\n",
        "numbers.append(df)\n",
        "dataset = pd.concat(numbers, ignore_index=True)\n",
        "values = dataset[[\"x1\", \"x2\"]].values\n",
        "distances = pdist(values)\n",
        "sigma = np.mean(distances)\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x - y)**2 / (2 * sigma**2))\n",
        "def create_spectral_graph(values, k, sigma):\n",
        "    num_points = len(values)\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(values)\n",
        "    distances, indices = knn.kneighbors(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j, idx in zip(range(k), indices[i]):\n",
        "            if i != idx:\n",
        "                weight_matrix[i, idx] = gaussian_kernel(values[i], values[idx], sigma)\n",
        "                weight_matrix[idx, i] = weight_matrix[i, idx]\n",
        "    return weight_matrix\n",
        "def spectral_laplacian_matrix(weight_matrix):\n",
        "    degree_matrix = np.diag(np.sum(weight_matrix, axis=1))\n",
        "    laplacian = degree_matrix - weight_matrix\n",
        "    return laplacian\n",
        "def spectral_second_eigenvector(laplacian):\n",
        "    eigenvalues, eigenvectors = eigh(laplacian)\n",
        "    return eigenvectors[:, 1]\n",
        "def spectral_dasgupta_cost(linkage_matrix, weight_matrix):\n",
        "    num_points = weight_matrix.shape[0]\n",
        "    total_cost = 0\n",
        "    for point1 in range(num_points):\n",
        "        for point2 in range(point1 + 1, num_points):\n",
        "            if weight_matrix[point1, point2] > 0:\n",
        "                lca_dist = node_distance(linkage_matrix, point1, point2, num_points)\n",
        "                total_cost += weight_matrix[point1, point2] * lca_dist\n",
        "    return total_cost\n",
        "def create_agglomerative_graph(values, sigma):\n",
        "    num_points = len(values)\n",
        "    weight_matrix = np.zeros((num_points, num_points))\n",
        "    for i in range(num_points):\n",
        "        for j in range(i + 1, num_points):\n",
        "            weight_matrix[i, j] = gaussian_kernel(values[i], values[j], sigma)\n",
        "            weight_matrix[j, i] = weight_matrix[i, j]\n",
        "    return weight_matrix\n",
        "agglomerative_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='average')\n",
        "agglomerative_clustering.fit(values)\n",
        "dataset[\"new_labels\"] = agglomerative_clustering.labels_\n",
        "linkage_matrix_agglomerative = agglomerative_clustering.children_\n",
        "weight_matrix_agglomerative = create_agglomerative_graph(values, sigma)\n",
        "cost_agglomerative = spectral_dasgupta_cost(linkage_matrix_agglomerative, weight_matrix_agglomerative)\n",
        "k_values = [3, 5, 10]\n",
        "cost_spectral_values = []\n",
        "for k in k_values:\n",
        "    weight_matrix_spectral = create_spectral_graph(values, k, sigma)\n",
        "    laplacian_spectral = spectral_laplacian_matrix(weight_matrix_spectral)\n",
        "    second_eig_spectral = spectral_second_eigenvector(laplacian_spectral)\n",
        "    linkage_matrix_spectral = linkage(values, method='ward')\n",
        "    cost_spectral = spectral_dasgupta_cost(linkage_matrix_spectral, weight_matrix_spectral)\n",
        "    cost_spectral_values.append(cost_spectral)\n",
        "data = {\"Method\": [\"Spectral Clustering (k=3)\", \"Spectral Clustering (k=5)\", \"Spectral Clustering (k=10)\", \"Agglomerative Clustering\"],\n",
        "    \"Dasgupta Cost\": cost_spectral_values + [cost_agglomerative]}\n",
        "df_result = pd.DataFrame(data)\n",
        "print(df_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJZD77GmvfYP",
        "outputId": "bd6b9a7c-e82a-4b51-dee4-233d07a821a6"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       Method  Dasgupta Cost\n",
            "0   Spectral Clustering (k=3)     309.872326\n",
            "1   Spectral Clustering (k=5)     561.719775\n",
            "2  Spectral Clustering (k=10)    1250.887268\n",
            "3    Agglomerative Clustering   17316.747367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### مشاهده میشود که مقدار تابع هزینه در الگوریتم خوشه بندی سلسله مراتبی طیفی به طور چشم گیری از الگوریتم تجمیعی بیشتر است"
      ],
      "metadata": {
        "id": "2y0060pNyGx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########الگوریتم سلسله مراتبی طبفی سریع تر عمل میکند در نتیجه میتوان گفت که این الگوریتم بهینه تر از الگوریتم تجمیعی میباشد"
      ],
      "metadata": {
        "id": "th6n7pAyyqFV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}